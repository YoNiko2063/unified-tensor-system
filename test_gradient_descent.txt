# Gradient Descent Optimization

## Abstract
We study the convergence of gradient descent for minimizing convex functions.

## Main Equation
The update rule is:
θₜ₊₁ = θₜ - α·∇L(θₜ)

Where α is the learning rate and L is the loss function.

## Theorem 1
For α < 1/L_smooth, gradient descent converges linearly:
‖θₜ - θ*‖ ≤ (1 - α·μ)ᵗ·‖θ₀ - θ*‖
