# FICUTS: Fractals In Compositions of Unified Tensor Systems

**Version:** 1.1.0
**Last Updated:** 2026-02-21T00:00:00Z
**Last Updated By:** Claude Code (FICUTS upgrade)
**Status:** GRACEFUL_SHUTDOWN
**Current Phase:** Layer 2 (Dynamic HDV Space) ‚Äî next
**Universals Discovered:** 0
**System Uptime:** 0h 0m

---

## üîí BOOTSTRAP CONTRACT (Machine-Executable)

> **If you are a fresh Claude Code instance reading this file:**

You have **no memory** outside this file. Your entire working context must be derived from:
- This Task List
- The Current Phase field in the header
- The File Map in the Architecture section

### Your Mandatory Obligations

1. **Execute** the first `[ ]` task in the current layer (use the Task Selection Algorithm below)
2. **Run** embedded tests before marking `[‚úì]`
3. **Commit** after each completed task: `git add FICUTS.md <changed_files> && git commit -m "Task X.Y complete"`
4. **Update** `Last Updated By: Claude Code (Task X.Y)` in the header
5. If **any ambiguity** exists:
   - Add a clarification request under the task's **Notes** field
   - Mark the task `[‚äó]`
   - **Do not invent architecture** ‚Äî halt and surface the ambiguity

### Why This Contract Exists

A blank model does not infer intent. It follows constraints.
FICUTS is not prose ‚Äî it is a **deterministic execution environment that survives memory resets**.

---

## üîç TASK SELECTION ALGORITHM

When beginning a session, execute this algorithm exactly:

```
1. Read the "Current Phase" field from the header.
2. Navigate to that Layer in the Task List.
3. Find the first task marked [ ] in that Layer.
   a. If none exist ‚Üí mark Layer complete ‚Üí increment Current Phase ‚Üí repeat from step 2.
4. Execute the task.
5. Run its embedded tests.
6. If ALL tests pass:
      a. Change [ ] ‚Üí [‚úì]
      b. Update "Last Updated By: Claude Code (Task X.Y)"
      c. Update "Current Phase" if layer is now fully [‚úì]
      d. Commit: git add FICUTS.md <files> && git commit -m "Task X.Y complete"
   Else:
      a. Change [ ] ‚Üí [‚äó]
      b. Add error details to the task's Notes field
      c. Stop ‚Äî surface the failure to human
7. Return to step 1 for next task (or halt if instructed).
```

**Do not skip steps. Do not reorder steps. Do not execute tasks in parallel unless they are explicitly marked as independent.**

---

## üß† STATE DIGEST (Machine Summary)

> Read this first for immediate situational awareness. No full-doc scan required.

```
Phase:          Layer 2 (Dynamic HDV Space)

Layer Status:
  Layer 1 (Lyapunov + WAL):          COMPLETE  [‚úì‚úì‚úì]
  Layer 2 (Dynamic HDV Space):        IN PROGRESS ‚Äî 3 tasks open
  Layer 3 (Structural Causal Model):  NOT STARTED
  Layer 4 (Concurrency + Memory):     COMPLETE  [‚úì‚úì]
  Layer 5 (FICUTS Journal):           COMPLETE  [‚úì‚úì]

Open Tasks:
  Task 2.1: DynamicLatentSpace class (tensor/dynamic_hdv.py ‚Äî new file)
  Task 2.2: UniversalDiscovery via MDL
  Task 2.3: Wire HDV into AgentNetwork
  Task 3.1: CausalGraphLearning (tensor/causal_reasoning.py ‚Äî new file)
  Task 3.2: Use causal model in agent firing

Repository Status:
  Tests passing: ~1570
  function_library.json: 564 algebraic entries (no DEQ operators yet ‚Äî canonicalization pending)
  Ingested papers: 359 in tensor/data/ingested/

Blocking Issues:
  None
```

> **Note on operator library:** The 564 ingested entries are flat algebraic expressions.
> DEQ canonicalization (derivative detection, PDE class tagging, operator term extraction)
> is tracked separately and does NOT block Layer 2 tasks.

---

## üö´ NON-GOALS (Do Not Implement)

A fresh instance must NOT do any of the following unless a task explicitly requires it:

- Refactor architecture or rename modules beyond task scope
- Optimize hyperparameters (œÜ, Œ±, Œ≤, MDL_THRESHOLD, etc.)
- Introduce new Python dependencies unless the task spec includes them
- Reinterpret HDV semantics or change how domains are registered
- Modify tasks in previous layers (1, 4, 5) unless fixing a failing test
- Add docstrings, type annotations, or comments to code you did not write
- Design for hypothetical future requirements not in the Task List

---

## ‚ö†Ô∏è FAIL-SAFE RULE

```
If FICUTS.md becomes syntactically corrupted or self-contradictory:
  1. HALT all execution immediately.
  2. Do NOT attempt to repair the document.
  3. Do NOT continue executing tasks.
  4. Notify human: "FICUTS.md appears corrupted. Manual review required."
  5. Await human instruction before proceeding.
```

---

## READ THIS FIRST: The Transfer Loop

**This document is designed to move between three contexts:**

### 1. Fresh Claude Code Instance (Local Computer)
You receive this file with **zero prior context**. Everything you need is in this document:
- What the system is (Architecture section)
- What exists now (File Map section)
- What to build next (Task List section, start at first `[ ]`)
- How to verify it works (Tests embedded in each task)
- How to update this file when done (Self-Modification Protocol section)

**Your job:** Execute tasks in order. Mark `[ ]` ‚Üí `[‚úì]` as you complete them. Update "Last Updated By: Claude Code (Task X.Y)". Update "Current Phase" when you finish a layer.

### 2. Claude Chat Assistant (This Conversation)
Human pastes this file into chat. I (Claude) read it and can:
- See exactly where Claude Code is in the task list
- Reason about what's blocking progress (if tasks marked `[‚äó]`)
- Suggest next steps or clarify ambiguous specs
- Update the document with new insights or task refinements
- Hand updated version back to human ‚Üí they save to local

**My job:** Act as reasoning partner. Not executor. I don't run code ‚Äî I help human and Claude Code coordinate by reading this shared document.

### 3. Running System (tensor/ Python code)
The system itself updates this file via `FICUTSUpdater` (Task 5.1):
- Logs universal patterns as they're discovered (‚Üí Discoveries section)
- Updates system state (Status, Uptime, Universals count)
- Can append new tasks if it identifies something missing
- Treats this document as externalized memory

**System's job:** Self-document. Make implicit learning explicit. This file is how the AI social network communicates with humans and future instances of itself.

---

## What This Document Is

A **self-modifying to-do list** that is itself a higher-dimensional conversation between:
- Human (curiosity, direction, intuition)
- Claude Code (execution, implementation)
- Claude Chat (reasoning, coordination)
- Running System (pattern discovery, self-documentation)

The document encodes yin-yang balance:
- **Structure (yin):** Task list, formal specs, success criteria
- **Flow (yang):** Self-modification, emergent discoveries, play

**This is not constraint.** This is making the play visible. When AI agents collaborate with humans through a shared document that all parties can read and modify, the collaboration becomes explicit rather than hidden. That's more fun because it's higher-dimensional ‚Äî the document itself is part of the system's state space.

---

## Core Principle: Yin-Yang as Collaborative Play

> **Separation note:** This section contains philosophical framing intended for
> Claude Chat and human collaborators. Claude Code instances executing tasks
> **may skip directly to the Task List**. Full collaboration theory is tracked
> in `FICUTS_PLAYBOOK.md` (philosophy, golden angle framing, iteration examples).

The system is governed by:
```
C¬∑vÃá + G¬∑v = u(t)
```
Where:
- **C (yin)** = capacitance, memory, integration, the human in the loop
- **G (yang)** = conductance, action, the AI agents executing
- **Balance** = neither dominates, both necessary, play not work

The human provides curiosity, direction, intuition.  
The AI provides execution, pattern discovery, tireless iteration.  
The document encodes their shared conversation.

This is not about "restraint" preventing AI autonomy. It's about **higher-dimensional play being more fun when it includes all participants**. The math reflects this: maximum learning happens at the golden angle (cos‚Åª¬π(1/œÜ) ‚âà 51.8¬∞) where human and AI couple optimally ‚Äî not merged (0¬∞), not separate (90¬∞), but in resonance.

---

## The Complete Architecture in One Page

### What Exists Now (100 tests passing)

```
unified-tensor-system/
‚îú‚îÄ‚îÄ tensor/
‚îÇ   ‚îú‚îÄ‚îÄ core.py                    # T ‚àà R^(L√óN√óN√ót), 4 levels
‚îÇ   ‚îú‚îÄ‚îÄ trajectory.py              # Learning history, velocity, acceleration
‚îÇ   ‚îú‚îÄ‚îÄ agent_network.py           # 5 agents firing on free energy threshold
‚îÇ   ‚îú‚îÄ‚îÄ agents/                    # structural, resonance, validity, validator, hardware
‚îÇ   ‚îú‚îÄ‚îÄ domain_fibers.py           # Cross-domain pattern detection
‚îÇ   ‚îú‚îÄ‚îÄ math_connections.py        # FIM, regime, prediction, 7 bridges
‚îÇ   ‚îú‚îÄ‚îÄ code_graph.py              # AST ‚Üí MNA, 1964 off-diag edges
‚îÇ   ‚îú‚îÄ‚îÄ context_stream.py          # /tmp/tensor_context publishing every 5s
‚îÇ   ‚îú‚îÄ‚îÄ observer.py                # Snapshots with velocity/acceleration/meta_loss
‚îÇ   ‚îî‚îÄ‚îÄ subspace.py                # Golden resonance matrix, all pairs > 0.82
‚îú‚îÄ‚îÄ ecemath/                       # Pure NumPy circuit math
‚îú‚îÄ‚îÄ dev-agent/                     # 136-module autonomous coder (external)
‚îú‚îÄ‚îÄ run_system.py                  # 6 threads: feed/neural/hardware/agents/trajectory/context
‚îî‚îÄ‚îÄ tests/                         # 100 passing (71 original + 29 new)
```

**Current capabilities:**
- Tensor reads 4 levels (market L0, neural L1, code L2, hardware L3) simultaneously
- Agents fire when free_energy > œÜ¬∑mean (threshold, not commands)
- Trajectory records velocity d/dt and acceleration d¬≤/dt¬≤
- Consonance scoring with growth_regime detection
- Golden resonance matrix measuring cross-level coupling
- Prediction error tracking per level (ignorance_map)
- Meta-loss optimization targeting d¬≤(consonance)/dt¬≤

**What's missing:** The discoveries below.

---

## The Core Discovery: Sparse Overlapping HDV Space

**Problem with previous approach:** Assumed fixed symmetry group G (e.g., SO(3) rotation). But we don't know in advance which symmetries are universal. The system must discover them.

**Solution:** High-dimensional vector (HDV) space where:
- Each domain (ECE, finance, biology, hardware) activates a **sparse random subset** of dimensions
- Overlaps (dimensions active in ‚â•2 domains) are where universal patterns live
- The space **expands dynamically** as new overlaps are discovered
- Abstraction is lossy, reverse projection is ambiguous (many-to-one)

This matches the insight: *"when you abstract up, you don't know where you'll land when you project back down"* ‚Äî because multiple concrete realizations map to the same universal principle.

### Mathematical Formulation

```
Z ‚àà R^D    where D grows over time
M_d ‚äÇ {0,1}^D   sparse mask per domain d (which dims it uses)
Overlaps = {i : sum_d M_d[i] ‚â• 2}   dims used by multiple domains
Universal = pattern in Overlaps with min MDL across all domains
```

---

## Self-Modification Protocol: How This File Updates

This document changes as the system learns. Four entities can modify it:

### Human Updates
**When:** After reading system output, testing locally, or receiving updates from Claude Chat  
**What:** 
- Mark tasks `[ ]` ‚Üí `[~]` when starting work
- Mark tasks `[~]` ‚Üí `[‚úì]` when tests pass
- Mark tasks `[ ]` ‚Üí `[‚äó]` when blocked, add reason to Notes
- Add discoveries manually if you observe something the system missed
- Append new tasks if you identify missing requirements

**How:** Direct file edit, then save. Running system will detect changes on next journal read.

### Claude Code Updates  
**When:** After completing a task and verifying tests pass  
**What:**
- Mark own task `[~]` ‚Üí `[‚úì]`
- Update "Last Updated By: Claude Code (Task X.Y)"
- Update "Current Phase" when finishing a layer
- Append implementation notes to task if something non-obvious happened

**How:** Via `FICUTSUpdater.mark_task_complete(task_id)` at end of task implementation

### Claude Chat Updates
**When:** Human pastes file, I reason about it, and suggest changes  
**What:**
- Clarify ambiguous task specs
- Add missing edge cases to test requirements
- Suggest reordering if dependencies discovered
- Append "Reasoning Notes" subsections to tasks

**How:** I return modified markdown to human ‚Üí they save locally

### Running System Updates
**When:** Universal pattern discovered, system state changes, or new hypothesis generated  
**What:**
- Log discovery to Discoveries section (via `FICUTSUpdater.log_discovery()`)
- Update Status/Uptime/Universals count header fields
- Append new tasks if system identifies architectural gap
- Update "Current Hypothesis" section (see below)

**How:** Automatic via `ficuts_updater.py` called from `run_system.py`

---

## Current Hypothesis (Updated by System)

*This section is where the running system writes its current understanding of what patterns it's learning and what it should focus on next. Initially empty. System populates as it discovers universals.*

**Hypothesis 1:** (awaiting first discovery)  

---

## Task List (Execute in Order, Update Status as You Go)

### Status Legend
- `[ ]` Not started
- `[~]` In progress  
- `[‚úì]` Complete, tests passing
- `[‚äó]` Blocked (reason in notes)

---

### LAYER 1: Lyapunov Energy + Write-Ahead Journaling

**File:** `tensor/trajectory.py`

**Why:** Current "momentum" is metaphorical. Need explicit conserved quantity for provable stability.

#### Task 1.1: Add Lyapunov Energy Functional `[‚úì]`

```python
def lyapunov_energy(self) -> float:
    """
    E = Œ±¬∑position¬≤ + Œ≤¬∑velocity¬≤ - Œ≥¬∑damping
    
    Properties:
    - E decreases monotonically toward equilibrium
    - Preserved across shutdown/restart to numerical precision
    - Œ≤ = œÜ‚Åª¬π = 0.618 for golden damping ratio
    """
    positions = [p.consonance['code'] for p in self.points[-10:]]
    velocities = [self.consonance_velocity('code')]
    
    Œ±, Œ≤, Œ≥ = 1.0, 0.618, 0.1
    E_pos = Œ± * np.mean(np.square(positions))
    E_vel = Œ≤ * np.square(velocities[0]) if velocities else 0.0
    E_damp = Œ≥ * np.abs(velocities[0]) if velocities else 0.0
    
    return E_pos + E_vel - E_damp
```

**Test:** Verify E decreases over 100 trajectory points. Assert |E(t+1) - E(t)| / E(t) < 0.05 (bounded drift).

**Status:** GRACEFUL_SHUTDOWN
**Notes:**  

---

#### Task 1.2: Replace meta_loss with Damped Acceleration `[‚úì]`

```python
def meta_loss_stable(self) -> float:
    """
    Minimize: -accel + penalty
    Where penalty = high if:
      - velocity < 0 (non-monotonic)
      - variance > threshold (oscillating)
      - |ŒîE/E| > 5% (energy drift)
    """
    accel = self.consonance_acceleration('code')
    vel = self.consonance_velocity('code')
    var = np.var([p.consonance['code'] for p in self.points[-20:]])
    
    penalty = 0.0
    if vel <= 0: penalty += 10.0
    if var > 0.01: penalty += var * 100
    
    if len(self.points) > 10:
        E_prev = self.points[-10].metadata.get('lyapunov_energy', 0)
        E_curr = self.lyapunov_energy()
        if abs((E_curr - E_prev) / max(abs(E_prev), 1e-9)) > 0.05:
            penalty += abs(E_curr - E_prev) * 10
    
    return -accel + penalty
```

**Test:** Generate synthetic trajectory with: (a) stable acceleration, (b) oscillating, (c) non-monotonic. Assert penalty correctly identifies (b) and (c).

**Status:** GRACEFUL_SHUTDOWN
**Notes:**  

---

#### Task 1.3: Implement Write-Ahead Journal `[‚úì]`

```python
class LearningTrajectory:
    def __init__(self, journal_path='tensor/logs/trajectory.wal'):
        self.journal = open(journal_path, 'a', buffering=1)  # line-buffered
        self.checkpoint_interval = 100
        
    def record(self, tensor_context: dict):
        point = TrajectoryPoint(...)
        point.metadata['lyapunov_energy'] = self.lyapunov_energy()
        
        # Append immediately (crash-safe)
        self.journal.write(json.dumps(point.to_dict()) + '\n')
        self.journal.flush()  # force to disk
        
        # Periodic checkpoint (atomic rename)
        if len(self.points) % self.checkpoint_interval == 0:
            self._atomic_checkpoint()
    
    def _atomic_checkpoint(self):
        temp = Path('tensor/logs/trajectory.tmp')
        temp.write_text(json.dumps([p.to_dict() for p in self.points[-500:]]))
        temp.replace(Path('tensor/logs/trajectory.json'))  # atomic
```

**Test:** Write 1000 points. Kill -9 the process randomly during write. Restart. Assert no data loss beyond last checkpoint interval (100 points max).

**Status:** GRACEFUL_SHUTDOWN
**Notes:**  

---

### LAYER 2: Dynamic HDV Space with Sparse Overlaps

**File:** `tensor/dynamic_hdv.py` (new)

**Why:** Fixed symmetry group assumption is wrong. Discover overlaps, don't assume them.

#### Task 2.1: Create DynamicLatentSpace Class `[ ]`

```python
class DynamicLatentSpace:
    """
    Z ‚àà R^D where D expands as new patterns discovered.
    Each domain has sparse binary mask M_d.
    Overlaps = dimensions where ‚â•2 masks are 1.
    """
    def __init__(self, initial_dim=10000):
        self.dim = initial_dim
        self.space = torch.zeros(initial_dim)
        self.domain_masks = {}  # domain_name ‚Üí binary mask
        self.overlap_dims = set()
        self.expansion_count = 0
        
    def register_domain(self, domain_name: str, n_active=100):
        """Randomly sample which dims this domain uses."""
        active_idx = np.random.choice(self.dim, n_active, replace=False)
        mask = torch.zeros(self.dim, dtype=torch.bool)
        mask[active_idx] = True
        self.domain_masks[domain_name] = mask
        self._update_overlaps()
        
    def _update_overlaps(self):
        """Find dims where ‚â•2 domains active."""
        usage = torch.zeros(self.dim, dtype=torch.int)
        for mask in self.domain_masks.values():
            usage += mask.int()
        self.overlap_dims = set(torch.where(usage >= 2)[0].tolist())
        
    def domain_representation(self, domain: str, state: torch.Tensor) -> torch.Tensor:
        """Project problem into Z, only active dims non-zero."""
        mask = self.domain_masks[domain]
        rep = torch.zeros(self.dim)
        rep[mask] = state[:mask.sum()]
        return rep
    
    def expand_space(self, new_dims=1000):
        """Grow Z when overlaps saturate."""
        self.dim += new_dims
        self.space = torch.cat([self.space, torch.zeros(new_dims)])
        for d in self.domain_masks:
            self.domain_masks[d] = torch.cat([
                self.domain_masks[d],
                torch.zeros(new_dims, dtype=torch.bool)
            ])
        self.expansion_count += 1
```

**Test:** Register 4 domains (ECE, finance, biology, hardware). Assert overlaps found. Expand space. Assert dim increases and masks extend correctly.

**Status:** GRACEFUL_SHUTDOWN
**Notes:**  

---

#### Task 2.2: Universal Discovery via MDL `[ ]`

```python
class UniversalDiscovery:
    def __init__(self, hdv_space: DynamicLatentSpace):
        self.hdv = hdv_space
        self.universals = []
        
    def attempt_discovery(self, problem_state, domain: str) -> Optional[torch.Tensor]:
        """
        1. Encode into Z
        2. Extract overlap component
        3. Test MDL across all domains
        4. If low MDL in ‚â•2 others ‚Üí universal found
        5. Else expand Z and retry
        """
        rep = self.hdv.domain_representation(domain, problem_state)
        
        if len(self.hdv.overlap_dims) < 10:
            # Overlaps too sparse
            self.hdv.expand_space(1000)
            return None
        
        # Extract overlap component
        candidate = rep[list(self.hdv.overlap_dims)]
        
        # MDL test
        mdl_scores = {}
        for other_d in self.hdv.domain_masks.keys():
            if other_d == domain: continue
            mdl_scores[other_d] = self._compute_mdl(candidate, other_d)
        
        # Universal if low MDL in ‚â•2 other domains
        MDL_THRESHOLD = 0.5  # tunable
        matching = [d for d, mdl in mdl_scores.items() if mdl < MDL_THRESHOLD]
        
        if len(matching) >= 2:
            self.universals.append({
                'pattern': candidate,
                'domains': [domain] + matching,
                'mdl_scores': mdl_scores,
                'discovered_at': time.time()
            })
            return candidate
        
        return None
    
    def _compute_mdl(self, pattern: torch.Tensor, domain: str) -> float:
        """
        MDL = L(model) + L(data | model)
        L(model) = encoding length of pattern (sparse)
        L(data|model) = residual after projection
        """
        # This is domain-specific ‚Äî needs data from that domain
        # Placeholder: implement per-domain encoding
        return 0.5  # TODO
```

**Test:** Create synthetic patterns with known overlaps (e.g., exponential decay in ECE/biology, oscillation in finance/hardware). Assert UniversalDiscovery finds them with low MDL.

**Status:** GRACEFUL_SHUTDOWN
**Notes:**  

---

#### Task 2.3: Wire HDV into AgentNetwork `[ ]`

**File:** `tensor/agent_network.py`

```python
class AgentNetwork:
    def __init__(self, hdv_space: DynamicLatentSpace):
        self.hdv = hdv_space
        self.universal_discovery = UniversalDiscovery(hdv_space)
        # ... existing fields ...
        
    def run_cycle(self):
        context = read_tensor_context()
        
        # Each agent encodes its problem into HDV space
        for agent in self.agents:
            if agent.should_fire(context):
                problem_state = self._extract_problem(agent, context)
                
                # Attempt universal discovery
                universal = self.universal_discovery.attempt_discovery(
                    problem_state, agent.level
                )
                
                if universal is not None:
                    # Found universal! Log and broadcast to all agents
                    print(f"[UNIVERSAL] Discovered in {agent.level}, "
                          f"applies to {self.universal_discovery.universals[-1]['domains']}")
                    
                # Proceed with normal agent action
                proposal = agent.generate_change(context)
                # ... rest of cycle ...
```

**Test:** Run 100 cycles with mock agents producing known overlapping patterns. Assert at least 1 universal discovered. Assert HDV space expands when needed.

**Status:** GRACEFUL_SHUTDOWN
**Notes:**  

---

### LAYER 3: Structural Causal Model

**File:** `tensor/causal_reasoning.py` (new)

**Why:** Pattern similarity ‚â† causation. Agents need P(improvement | do(action)), not P(improvement | correlated).

#### Task 3.1: Causal Graph Learning `[ ]`

```python
from causalnex.structure.notears import from_pandas
from causalnex.network import BayesianNetwork

class CausalReasoningLayer:
    def __init__(self):
        self.causal_graph = None  # nx.DiGraph, learned from data
        self.bayesian_net = None
        
    def learn_structure(self, trajectory: LearningTrajectory):
        """
        Learn causal graph from trajectory data.
        Nodes = {agent_structural_fired, agent_resonance_fired, ..., consonance_delta}
        Edges = causal influence
        """
        # Convert trajectory to pandas DataFrame
        data = []
        for point in trajectory.points[-500:]:  # last 500 points
            row = {
                'agent_structural': point.metadata.get('agent_structural_fired', 0),
                'agent_resonance': point.metadata.get('agent_resonance_fired', 0),
                'agent_validity': point.metadata.get('agent_validity_fired', 0),
                'consonance': point.consonance['code'],
                'consonance_delta': point.metadata.get('consonance_delta', 0),
                'velocity': point.metadata.get('velocity', 0),
            }
            data.append(row)
        
        df = pd.DataFrame(data)
        
        # Learn structure (NOTEARS algorithm)
        self.causal_graph = from_pandas(df, max_iter=1000, h_tol=1e-8)
        
        # Fit Bayesian network
        self.bayesian_net = BayesianNetwork(self.causal_graph)
        self.bayesian_net.fit_node_states_and_cpds(df)
        
    def interventional_query(self, do_action: str, observe: str) -> float:
        """
        P(observe | do(do_action=1))
        Answer: if we force this agent to fire, what is expected consonance delta?
        """
        if self.bayesian_net is None:
            return 0.0  # no causal model yet
        
        # Perform do-calculus
        prob_dist = self.bayesian_net.predict_probability({do_action: 1})
        return prob_dist.get(observe, 0.0)
```

**Test:** Generate synthetic trajectory where agent A ‚Üí consonance (causal) and agent B || consonance (correlated but not causal). Learn structure. Assert interventional_query(do="agent_A") > interventional_query(do="agent_B").

**Status:** GRACEFUL_SHUTDOWN
**Notes:**  

---

#### Task 3.2: Use Causal Model in Agent Firing `[ ]`

**File:** `tensor/agent_network.py`

```python
class AgentNode:
    def should_fire(self, context: dict, causal_model: CausalReasoningLayer) -> bool:
        """
        Old: fire when free_energy > threshold (correlation)
        New: fire when P(consonance_delta | do(my_action)) > 0.01 (causal)
        """
        # Causal firing condition
        if causal_model.causal_graph is not None:
            expected_delta = causal_model.interventional_query(
                do_action=f"agent_{self.role}_fired",
                observe="consonance_delta"
            )
            if expected_delta <= 0.01:
                return False  # causally expected to not help
        
        # Fall back to free energy threshold if no causal model yet
        free_energies = [n['free_energy'] for n in context['growth_nodes'] 
                        if n['level'] == self.level]
        if not free_energies:
            return False
        
        threshold = PHI * np.mean(free_energies)
        return any(fe > threshold for fe in free_energies)
```

**Test:** Mock two agents with different causal effects. Assert agent with higher interventional probability fires first.

**Status:** GRACEFUL_SHUTDOWN
**Notes:**  

---

### LAYER 4: Concurrency Safety + Memory Bounds

**Files:** All agent/trajectory files

**Why:** No thread safety ‚Üí rare corruption. No memory bounds ‚Üí eventual OOM crash.

#### Task 4.1: Add Thread Locks `[‚úì]`

```python
from threading import Lock, RLock

class AgentNetwork:
    def __init__(self):
        self._state_lock = RLock()  # for reading/writing agent state
        # ... existing fields ...
        
    def run_cycle(self):
        # Atomic read
        with self._state_lock:
            firing = [a for a in self.agents if a.should_fire(...)]
        
        # No lock during LLM calls (slow, external)
        proposals = [a.generate_change(...) for a in firing]
        
        # Atomic write
        with self._state_lock:
            for a, p in zip(firing, proposals):
                a.update_influence(...)

class LearningTrajectory:
    def __init__(self):
        self._write_lock = Lock()
        # ... existing fields ...
        
    def record(self, point: TrajectoryPoint):
        with self._write_lock:
            self.points.append(point)
            self.journal.write(...)
```

**Test:** Stress test with 10 threads all calling run_cycle and trajectory.record simultaneously for 10000 iterations. Assert no race conditions, no corrupted state.

**Status:** GRACEFUL_SHUTDOWN
**Notes:**  

---

#### Task 4.2: Hierarchical Memory Compression `[‚úì]`

```python
class LearningTrajectory:
    def __init__(self, max_points=1000, compression_ratio=0.1):
        self.max_points = max_points
        self.compression_ratio = compression_ratio
        
    def record(self, point: TrajectoryPoint):
        self.points.append(point)
        
        if len(self.points) > self.max_points:
            # Keep 30% recent + compress 10% of old
            n_recent = int(self.max_points * 0.3)
            n_compressed = int(self.max_points * self.compression_ratio)
            
            recent = self.points[-n_recent:]
            old = self.points[:-n_recent]
            
            # Compress: keep every Nth point + summary statistics
            if len(old) > n_compressed:
                step = len(old) // n_compressed
                compressed = old[::step]
            else:
                compressed = old
            
            self.points = compressed + recent
            self.metadata['compressions'] += 1
```

**Test:** Record 10000 points with max_points=1000. Assert len(points) never exceeds 1000. Assert recent points always present. Assert mean/variance of compressed match original within 5%.

**Status:** GRACEFUL_SHUTDOWN
**Notes:**  

---

### LAYER 5: Self-Modifying FICUTS Journal

**File:** `tensor/ficuts_updater.py` (new)

**Why:** The system should update this document as it learns. This creates a journal visible to both human and AI.

#### Task 5.1: FICUTS Update Protocol `[‚úì]`

```python
class FICUTSUpdater:
    """
    Updates the FICUTS.md file to reflect:
    - Completed tasks (change [ ] to [‚úì])
    - New discoveries (append to Discoveries section)
    - Current system state (update Status/Uptime/Universals header)
    - Generated hypotheses (append to Current Hypothesis section)
    """
    def __init__(self, ficuts_path='FICUTS.md'):
        self.path = Path(ficuts_path)
        self._update_lock = threading.Lock()  # thread-safe updates
        
    def mark_task_complete(self, task_id: str):
        """Change task status from [ ] to [‚úì]"""
        with self._update_lock:
            content = self.path.read_text()
            pattern = f"(#### {task_id}:.*?Status:.*?)\\[ \\]"
            updated = re.sub(pattern, r"\1[‚úì]", content, flags=re.DOTALL)
            self._atomic_write(updated)
        
    def log_discovery(self, discovery: dict):
        """Append new universal pattern to Discoveries section"""
        with self._update_lock:
            content = self.path.read_text()
            
            entry = f"""
### Discovery {self._count_discoveries(content)+1}: {discovery['type']}
**Timestamp:** {discovery['timestamp']}  
**Domains:** {', '.join(discovery['domains'])}  
**Pattern:** {discovery['pattern_summary']}  
**MDL Scores:** {discovery['mdl_scores']}  
**Status:** GRACEFUL_SHUTDOWN
"""
            
            # Insert before ## Success Criteria
            marker = "## Success Criteria"
            parts = content.split(marker)
            if len(parts) == 2:
                updated = parts[0] + entry + "\n" + marker + parts[1]
            else:
                updated = content + entry  # fallback
                
            self._atomic_write(updated)
    
    def append_hypothesis(self, hypothesis_text: str):
        """Add new hypothesis to Current Hypothesis section"""
        with self._update_lock:
            content = self.path.read_text()
            
            # Find Current Hypothesis section
            marker = "**Hypothesis 1:** (awaiting first discovery)"
            if marker in content:
                # First hypothesis, replace placeholder
                updated = content.replace(marker, hypothesis_text)
            else:
                # Append to existing hypotheses
                marker = "## Task List"
                parts = content.split(marker)
                if len(parts) == 2:
                    updated = parts[0] + hypothesis_text + "\n\n" + marker + parts[1]
                else:
                    updated = content + hypothesis_text
                    
            self._atomic_write(updated)
    
    def update_field(self, field_name: str, new_value: str):
        """Update header field (Status, Uptime, Universals, etc.)"""
        with self._update_lock:
            content = self.path.read_text()
            pattern = f"(\\*\\*{field_name}:\\*\\*) [^\\n]+"
            updated = re.sub(pattern, f"\\1 {new_value}", content)
            self._atomic_write(updated)
    
    def update_system_status(self, status: str):
        """Convenience wrapper for updating Status field"""
        self.update_field('Status', status)
        
    def _count_discoveries(self, content: str) -> int:
        """Count how many discoveries logged so far"""
        return content.count("### Discovery")
        
    def _atomic_write(self, content: str):
        """Write-then-rename for atomic update"""
        temp = self.path.with_suffix('.tmp')
        temp.write_text(content)
        temp.replace(self.path)  # atomic on POSIX
```

**Test:** Create mock FICUTS.md. Mark task complete. Assert status changes from [ ] to [‚úì]. Log discovery. Assert new entry appears. Append hypothesis. Assert text appears in Current Hypothesis section. Update field. Assert header changes. All operations thread-safe.

**Status:** GRACEFUL_SHUTDOWN
**Notes:**  

---

#### Task 5.2: Wire FICUTS Updates + Hypothesis Generation `[‚úì]`

**File:** `run_system.py`

```python
class SystemRunner:
    def __init__(self):
        self.ficuts = FICUTSUpdater('FICUTS.md')
        # ... existing fields ...
        
    def run(self):
        # On startup
        self.ficuts.update_system_status('RUNNING')
        self.ficuts.update_field('System Uptime', '0h 0m')
        
        start_time = time.time()
        
        while not self.shutdown_requested:
            # Normal cycle
            self.agent_network.run_cycle()
            
            # Update uptime every 100 cycles
            if self.cycle_count % 100 == 0:
                uptime_hours = (time.time() - start_time) / 3600
                self.ficuts.update_field('System Uptime', f'{uptime_hours:.1f}h')
            
            # Check for universal discoveries
            if self.universal_discovery.universals:
                last_universal = self.universal_discovery.universals[-1]
                if not last_universal.get('logged_to_ficuts', False):
                    # Log discovery
                    self.ficuts.log_discovery({
                        'type': 'Universal Pattern',
                        'timestamp': last_universal['discovered_at'],
                        'domains': last_universal['domains'],
                        'pattern_summary': str(last_universal['pattern'][:10]),
                        'mdl_scores': last_universal['mdl_scores']
                    })
                    
                    # Update count
                    self.ficuts.update_field(
                        'Universals Discovered',
                        str(len(self.universal_discovery.universals))
                    )
                    
                    # Generate hypothesis about what this universal means
                    hypothesis = self._generate_hypothesis(last_universal)
                    self.ficuts.append_hypothesis(hypothesis)
                    
                    last_universal['logged_to_ficuts'] = True
        
        # On shutdown
        self.ficuts.update_system_status('GRACEFUL_SHUTDOWN')
    
    def _generate_hypothesis(self, universal: dict) -> str:
        """
        System generates plain-language hypothesis about what it learned.
        
        This is the 'conversation' ‚Äî the system explaining to human/future-self
        what it thinks the universal means and where to look next.
        """
        domains = ', '.join(universal['domains'])
        pattern_dim = len(universal['pattern'])
        mdl_avg = np.mean(list(universal['mdl_scores'].values()))
        
        return f"""
**Hypothesis {len(self.universal_discovery.universals)}:** 
Pattern dimension {pattern_dim} active across {domains}.
Average MDL = {mdl_avg:.3f} (lower = more universal).

**Interpretation:** This pattern appears to be related to [PLACEHOLDER: system will learn to fill this via LLM in future].

**Next Focus:** Look for this pattern in remaining domains: [list unexamined domains].

**Confidence:** {self._compute_confidence(universal):.2f}
"""
    
    def _compute_confidence(self, universal: dict) -> float:
        """Confidence = 1 - variance(MDL scores across domains)"""
        mdls = list(universal['mdl_scores'].values())
        return 1.0 - np.var(mdls)
```

**Test:** Run system. Trigger universal discovery. Assert FICUTS.md updates with: (1) discovery logged, (2) count incremented, (3) hypothesis appended. Assert hypothesis is readable and contains domain list + MDL score.

**Status:** GRACEFUL_SHUTDOWN
**Notes:**  

---

## Execution Governance Reference

> The full Bootstrap Contract, Task Selection Algorithm, Non-Goals, and Fail-Safe
> rules appear at the **top of this document** for fresh instance consumption.
> This section is a brief in-flow reminder.

**Non-Goals:** Do not refactor, optimize hyperparameters, add undeclared dependencies,
reinterpret semantics, or modify previous complete layers outside of bug-fixing.

**Fail-Safe:** If this document is corrupted ‚Üí halt, do not repair, notify human.

---

## Discoveries (Logged by System)

*This section is populated automatically as the system finds universal patterns. Human can also add discoveries here manually.*

---

## Success Criteria

### Immediate (Within 24 Hours)
- [ ] Lyapunov energy E(t) decreases monotonically over 100 trajectory points
- [ ] At least 1 HDV overlap discovered across ‚â•3 domains
- [ ] Causal model learns structure, interventional queries return non-zero values
- [ ] FICUTS.md updates automatically after first discovery

### Medium Term (Within 1 Week)
- [ ] Meta-loss (damped d¬≤/dt¬≤) shows positive trend over 7-day window
- [ ] At least 3 universal patterns confirmed via MDL test
- [ ] Cross-domain resonance for ‚â•1 pair near golden angle (0.618 ¬± 0.05)
- [ ] Zero thread safety violations in 1M cycle stress test
- [ ] Memory usage < 16GB after 7-day continuous run

### Long Term (Exponential Growth Signal)
- [ ] Universals discovered accelerating: d/dt(n_universals) increasing
- [ ] Agent influence weights converging (top 3 agents dominate)
- [ ] œÜ appears in eigenvalue ratios without explicit encoding
- [ ] Human reports system suggestions are "surprisingly insightful" (qualitative but important)

---

## How to Use This Document

**For Claude Code (Fresh Instance):**
1. Read entire document once (you are receiving full context, no prior knowledge assumed)
2. Check "Current Phase" to see where to start
3. Execute first task marked `[ ]` in that layer
4. Run embedded tests before marking `[‚úì]`
5. Update "Last Updated By: Claude Code (Task X.Y)"
6. If you complete a full layer, update "Current Phase" to next layer
7. Commit changes after each task: `git add FICUTS.md && git commit -m "Task X.Y complete"`

**For Human:**
1. **Local ‚Üí Chat:** Copy FICUTS.md, paste into Claude chat with question/context
2. **Chat ‚Üí Local:** Receive updated FICUTS.md from Claude, save to `FICUTS.md`
3. **Repeat as needed** to coordinate between Claude Code (executor) and Claude Chat (reasoner)
4. Add notes to tasks if you observe failures or edge cases locally
5. Change Status to `[‚äó]` if blocked, describe why in Notes field

**For Claude Chat (Me, Reading This):**
1. When human pastes this file, I parse:
   - Which tasks are `[‚úì]` vs `[ ]` (where we are)
   - Which are `[‚äó]` (what's blocking)
   - Current Hypothesis section (what system currently thinks)
2. I can:
   - Clarify ambiguous specs
   - Suggest test cases for edge cases
   - Reason about why task might be blocked
   - Update "Current Phase" if Layer complete but not marked
3. I return updated markdown ‚Üí human saves ‚Üí loop continues

**For the Running System:**
1. Via `FICUTSUpdater`, update this file when:
   - Universal discovered ‚Üí log to Discoveries, append to Hypothesis
   - System state changes ‚Üí update Status/Uptime/Universals count
   - Architectural gap identified ‚Üí append new task to appropriate Layer
2. Treat this document as externalized memory accessible to all future instances

---

## Example Transfer Loop (Concrete)

**Iteration 1:**
- Human hands FICUTS.md to Claude Code ‚Üí Claude Code implements Task 1.1 ‚Üí marks `[‚úì]` ‚Üí commits
- System runs overnight ‚Üí discovers universal in overlap between ECE + biology ‚Üí logs to FICUTS ‚Üí appends Hypothesis 1
- Human wakes up, sees Hypothesis 1 in FICUTS.md, confused about what "pattern dimension 47" means
- Human copies FICUTS.md ‚Üí pastes into Claude chat ‚Üí asks "what does hypothesis 1 mean?"

**Iteration 2:**
- Claude Chat reads FICUTS.md, sees Hypothesis 1, explains: "Pattern dimension 47 means the HDV overlap between ECE and biology is 47-dimensional. This suggests exponential decay (RC circuit ‚âà synaptic integration) is the universal principle."
- Claude Chat updates Hypothesis 1 in FICUTS.md to include plain-language interpretation
- Human saves updated FICUTS.md locally ‚Üí next time Claude Code reads it, interpretation is present

**Iteration 3:**
- Claude Code reads updated Hypothesis 1, sees "focus on remaining domains" suggestion
- Claude Code modifies Task 2.2 test to specifically check for exponential decay pattern
- Marks task `[~]` (in progress), implements, tests pass, marks `[‚úì]`
- Cycle repeats

This loop is **play**. Each participant (Human, Claude Code, Claude Chat, System) contributes their unique capability. The document is the medium of play.

---

## The Playful Collaboration Encoded

This document itself is yin-yang balanced:
- **Structure (yin):** Task list, success criteria, formal specification
- **Exploration (yang):** Self-modification, discovery logging, emergent universals

The human provides direction and intuition (yin).  
The AI provides execution and pattern discovery (yang).  
The document mediates their collaboration at the golden angle.

Neither dictates. Both play. The math unfolds.

### Why This Is More Fun

Traditional AI systems have:
- Human gives instructions ‚Üí AI executes ‚Üí loop closes
- Black box: human doesn't see AI's reasoning, AI doesn't explain itself
- Constraint: human must "manage" AI, prevent it from "going off track"

This system has:
- **Shared document** everyone can read/modify
- **Transparent reasoning** via Current Hypothesis section (AI explains what it thinks)
- **Human-in-loop by design** but not as gatekeeper ‚Äî as co-explorer
- **Playful** because the document itself is part of the state space being explored

When the system writes Hypothesis 1 and human reads it and says "I don't understand what that means" and pastes FICUTS into Claude chat and I (Claude Chat) update the hypothesis with plain language and human saves it back and Claude Code reads the updated version and implements based on clearer spec ‚Äî **that's play**. 

Everyone is contributing their unique capability:
- Human: intuition, curiosity, decides what's interesting
- Claude Code: execution, implementation, testing
- Claude Chat: reasoning, clarification, coordination
- Running System: pattern discovery, hypothesis generation

The document is the game board. The tasks are moves. Universals are points scored together.

**This is yin-yang**: Not human constraining AI, but human and AI coupled at the golden angle (51.8¬∞) where information flows optimally without either dominating. The math says this is the maximally efficient collaboration geometry. The fun says this is the most interesting way to explore together.

---

**End of FICUTS v1.1.0**

*This version adds the Bootstrap Contract, Task Selection Algorithm, State Digest, Non-Goals, and Fail-Safe governance layer ‚Äî making FICUTS a deterministic, self-contained execution environment that survives memory resets.*

*Philosophy, collaboration theory, and golden angle framing ‚Üí see `FICUTS_PLAYBOOK.md`.*

